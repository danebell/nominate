{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "from math import fsum, log\n",
    "import operator\n",
    "from glob import glob\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "curr_year = date.today().year\n",
    "\n",
    "# must end in '/'\n",
    "corpus_loc = \"../names/\"\n",
    "ngram_data_loc = \"./ngram_data/\"\n",
    "\n",
    "# scaling factor for discounting old names; the higher, the more discounted\n",
    "# range: 0 to infinity\n",
    "ago_scaling= 0.25\n",
    "\n",
    "# how many characters to condition on + 1\n",
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chop_row(row, n):\n",
    "    buffered = ((n - 1) * '<') + row.text + '>'\n",
    "    return [buffered[i:i+n] for i in range(len(buffered) - n + 1)]\n",
    "    \n",
    "def chop_df(df, n=3):\n",
    "    return df.apply(lambda x: chop_row(x, n), axis=1)\n",
    "\n",
    "def normalize(d):\n",
    "    factor = 1.0 / fsum(d.values())\n",
    "    return { k: v * factor for k, v in d.items() }\n",
    "\n",
    "def sum_to_one(p):\n",
    "    probs = np.array(p)\n",
    "    probs /= probs.sum()\n",
    "    probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the probability tables per ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = dict()\n",
    "fcounts = dict()\n",
    "mcounts = dict()\n",
    "names = set()\n",
    "fnames = set()\n",
    "mnames = set()\n",
    "\n",
    "prog = FloatProgress(min=0, max=len(glob(corpus_loc + \"*.txt\")))\n",
    "display(prog)\n",
    "\n",
    "for f in glob(corpus_loc + \"*.txt\"):\n",
    "    ago = curr_year - int(f.replace(corpus_loc, \"\").replace(\"yob\", \"\").replace(\".txt\", \"\"))\n",
    "        \n",
    "    df = pd.read_csv(f, names=['text','gender','freq'], sep=',', index_col=False)\n",
    "        \n",
    "    for (name, gender) in zip(df.text, df.gender):\n",
    "        names.add(name)\n",
    "        if gender == 'F':\n",
    "            fnames.add(name)\n",
    "        if gender == 'M':\n",
    "            mnames.add(name)\n",
    "\n",
    "    for (arr, freq, gender) in zip(chop_df(df, n), df.freq, df.gender):\n",
    "        for ngram in arr:\n",
    "            counts[ngram] = counts.get(ngram, 0) + (freq / np.power((ago + 1), ago_scaling))\n",
    "            if gender == 'F':\n",
    "                fcounts[ngram] = fcounts.get(ngram, 0) + (freq / np.power((ago + 1), ago_scaling))\n",
    "            if gender == 'M':\n",
    "                mcounts[ngram] = mcounts.get(ngram, 0) + (freq / np.power((ago + 1), ago_scaling))\n",
    "    prog.value += 1\n",
    "\n",
    "pickle.dump(counts, open(ngram_data_loc + 'counts' + str(n) + '.p', \"wb\"))\n",
    "pickle.dump(fcounts, open(ngram_data_loc + 'fcounts' + str(n) + '.p', \"wb\"))\n",
    "pickle.dump(mcounts, open(ngram_data_loc + 'mcounts' + str(n) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the probability tables once made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ngram_data/mcounts4.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bb95b7c1f303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_data_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'counts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_data_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'fcounts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_data_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mcounts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ngram_data/mcounts4.p'"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "\n",
    "counts = pickle.load(open(ngram_data_loc + 'counts' + str(n) + '.p', 'rb'))\n",
    "fcounts = pickle.load(open(ngram_data_loc + 'fcounts' + str(n) + '.p', 'rb'))\n",
    "mcounts = pickle.load(open(ngram_data_loc + 'mcounts' + str(n) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deep = dict()\n",
    "fdeep = dict()\n",
    "mdeep = dict()\n",
    "\n",
    "for k in counts:\n",
    "    cond = k[0:(n-1)]\n",
    "    consequent = k[(n-1):]\n",
    "    if not cond in deep:\n",
    "        deep[cond] = dict()\n",
    "    deep[cond][consequent] = counts[k]\n",
    "\n",
    "for k in fcounts:\n",
    "    cond = k[0:(n-1)]\n",
    "    consequent = k[(n-1):]\n",
    "    if not cond in fdeep:\n",
    "        fdeep[cond] = dict()\n",
    "    fdeep[cond][consequent] = fcounts[k]\n",
    "\n",
    "for k in mcounts:\n",
    "    cond = k[0:(n-1)]\n",
    "    consequent = k[(n-1):]\n",
    "    if not cond in mdeep:\n",
    "        mdeep[cond] = dict()\n",
    "    mdeep[cond][consequent] = mcounts[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # add a small value for combinations not seen, rather than backing off\n",
    "\n",
    "# alphabet = set(list(''.join(names).lower())).union(set('>'))\n",
    "\n",
    "# for karr in itertools.permutations(alphabet, r = n - 1):\n",
    "#     k = ''.join(karr)\n",
    "#     if k not in deep:\n",
    "#             deep[k] = dict()\n",
    "\n",
    "# for k in deep:\n",
    "#     if k != ((n - 1) * \"<\"):\n",
    "#         for a in alphabet:\n",
    "#             deep[k][a] = deep[k].get(a, 0) + 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now normalize\n",
    "\n",
    "for k in deep:\n",
    "    deep[k] = normalize(deep[k])\n",
    "\n",
    "for k in fdeep:\n",
    "    fdeep[k] = normalize(fdeep[k])\n",
    "\n",
    "for k in mdeep:\n",
    "    mdeep[k] = normalize(mdeep[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annifer, Astrel, Kell, Phyline, Karet, Wand, Nicoletty, Gildrey, Rachellie, Kaylorah, Brancesca, Audreda, Carrieanna, Marlotte, Kyriquelina, Abigailey, Josita, Claudith, Fernee, Leth, Cecellen, Barbarah, Blonice, Bonnah, Stephian\n",
      "Lore, Micharles, Salake, Richael, Dant, Dome, Christonio, Michard, Stevelt, Roques, Marker, Richell, Zacheal, Emmed, Yohampton, Westophen, Richael, Lawrent, Josepher, Clyden, Denn, Reynar, Tristopher, Larrentin, Jero\n",
      "Jessicarley, Kimbert, Karence, Sebanie, Patri, Karette, Cheyenneth, Maristophen, Avonnielle, Felie, Antjuanie, Aliyanaya, Harmenee, Dennegan, Brank, Vaneth, Jeres, Nancine, Wylerica, Denix, Coope, Earles, Andredo, Benja, Andreano\n"
     ]
    }
   ],
   "source": [
    "howmany = 25\n",
    "maxlength = 50\n",
    "\n",
    "def generate(howmany, gender, maxlength):\n",
    "    newnames = list()\n",
    "    if (gender == \"F\"):\n",
    "        cond = fdeep\n",
    "        extant = fnames\n",
    "    elif (gender == \"M\"):\n",
    "        cond = mdeep\n",
    "        extant = mnames\n",
    "    else:\n",
    "        cond = deep\n",
    "        extant = names\n",
    "\n",
    "    for i in range(howmany):\n",
    "        new = False\n",
    "        sofar = \"<\" * (n - 1)\n",
    "        while (not new):\n",
    "            given = cond[sofar[(1-n):]]\n",
    "            proposed = np.random.choice(list(given.keys()), 1, False, list(given.values()))[0]\n",
    "            if proposed == \">\" or len(sofar[(n-1):]) >= maxlength:\n",
    "                if sofar[(n-1):] not in extant:\n",
    "                    newnames.append(sofar[(n-1):])\n",
    "                    new = True\n",
    "                sofar = \"<\" * (n - 1)\n",
    "            else:\n",
    "                sofar = sofar + proposed\n",
    "                new = False\n",
    "    return(newnames)\n",
    "    \n",
    "newnames = generate(howmany, \"F\", maxlength)\n",
    "print(\"{}\".format(\", \".join(newnames)))\n",
    "\n",
    "newnames = generate(howmany, \"M\", maxlength)\n",
    "print(\"{}\".format(\", \".join(newnames)))\n",
    "\n",
    "newnames = generate(howmany, \"\", maxlength)\n",
    "print(\"{}\".format(\", \".join(newnames)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
